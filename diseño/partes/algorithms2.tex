
\subsection{Otros Algoritmos} % (fold)
\label{sub:otros_algoritmos}

\subsubsection{Random Forest} % (fold)
\label{ssub:random_forest}

Este algoritmo, conocido en castellano como ``Selvas Aleatorias'' es un algoritmo de ensamblado que combina varios algoritmos \textit{debiles} (por si solos no logran una clasificación eficiente) para construir un resultado mejor. Random Forest consta de varios arboles de predicción que se entrenan de forma independiente, cada uno con un subset de features de los datos de entrada - de esta manera se obtiene una colección de arboles no correlacionados. Para obtener la predicción final, los resultados de cada arbol se promedian. Para obtener resultados se le pueden variar la cantidad de arboles a usar, la cantidad de features por arbol (suele ser bastante más chica que la cantidad de features total, como la raiz cuadrada de ella, por ejemplo) y la profundidad de cada arbol.

El concepto es interesante, pero su aplicación con nuestro set falló en la práctica. Hemos hecho pruebas con las implementaciones de Random Forest de \textbf{scikit}. Dió excelentes resultados sobre el set de entrenamiento, primero entrenado, y luego con las predicciones sobre subsets de los datos de entrenamiento se obtiene un score mayor a 90\% y un \textit{logloss} de \~0.4. Pero se equivoca mucho sobre los datos que no conoce - al entrenarlo con un subset de entrenamiento y probarlo con otro subset, el score se vuelve muy bajo y el \textit{logloss} altisimo. La submisión a Kaggle nos resultó en un \textit{logloss} de 19.25726. Cabe destacar que solo lo hemos probado 1 vez, con 10 arboles y sin limitarle la profundidad a cada arbol - tendríamos que hacer pruebas variando esos parametros. Acá nos enfrentamos con otro problema - la implemetación de \textit{scikit} de RandomForest consume demasiada memoria RAM - dicho entrenamiento y posterior predicción usaron los 6GB de una notebook. Por otro lado, la aplicación fue muy rápida, tardando unos pocos minutos.

% subsubsection random_forest (end)

\subsection{Predicción sin algoritmo} % (fold)
\label{sub:prediccion_sin_algoritmo}

La predicción sin algoritmo tambien es un enfoque posible - y es bastante sencillo, es el camino que tomamos al principio para entender mejor de que se trata la competencia. La idea es realizar una predicción basada en estadísticas y promedios del set de entrenamiento. Por ejemplo, asumir que la probabilidad de cada crimen de una fila del set de prueba es igual al procentaje de cada crímen en el distrito de esa fila. Ese porcentaje es calculado anteriormente sobre el set de entrenamiento - basicamente es el total de tipo de crímen sobre el total de crimenes en el distrito. Es interesante que este método, solo con el distrito, recibió un \textit{logloss} de 2.65400 en Kaggle.

Tal resultado obviamente se debe a la distribución variada de crímenes donde unos pocos tipos de crímen conforman el mayor porcentaje, y la puntuación va a ser alta mientras esos crímenes tengan mayor probabilidad. Sin embargo, seguramente el resultado va a mejorar agregando más estadísticas. Este enfoque tiene sus problemáticas, como el descarte de los falsos negativos, pero es rápido y fácil. Se puede tomar como piso y tratar de mejorar sus resultados aplicando las prácticas de Machine Learning.

% subsection  prediccion_sin_algoritmo(end)


% subsection otros_algoritmos (end)
